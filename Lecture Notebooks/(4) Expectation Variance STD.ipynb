{"nbformat":4,"nbformat_minor":0,"metadata":{"celltoolbar":"Slideshow","kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.7"},"colab":{"name":"Copy of 04_expectation_variance_std.ipynb","provenance":[{"file_id":"1MgbxmGCYyk7TsQe0ATO1ZSzbMIWGt7-m","timestamp":1634765275651}]}},"cells":[{"cell_type":"code","metadata":{"id":"bFdQ_HyzmkFx"},"source":["# code to produce mystery sequence of integers stored in a list\n","\n","def mystery_sequence(size=10, lam=1.89):\n","    '''\n","    lam = 1.89 is the expectation value for the number of rainy days in early October in one week\n","    '''\n","    import numpy as np\n","\n","    return list(np.random.poisson(size=size, lam=lam))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"egOfMyi8mkF0"},"source":["<center>\n","\n","## <font color='maroon'>ASTR 20500</font>\n","\n","## <font color='maroon'>Expectation value, sample mean, variance, and standard deviation\n","## <font color='maroon'>of probability distributions\n","\n","   \n"," </center>\n","<p><p>\n","\n","<p>\n","<center><img align=center width=800 src=\"https://upload.wikimedia.org/wikipedia/commons/0/0c/Determination_of_the_rute_and_the_feet_in_Frankfurt.png\"></img>\n","\n"]},{"cell_type":"code","metadata":{"id":"_WGzckjjmkF2","outputId":"7dd58f25-3348-42a5-dd6e-7ef922baa7d9"},"source":["a = [1, 2, 3, 4, 5]\n","b = a\n","b = [2, 3, 4]\n","print(a is b)"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["False\n"]}]},{"cell_type":"code","metadata":{"id":"vG1GpzXLmkF2","outputId":"b24a424d-0984-405c-d4f6-4adca12995b1"},"source":["a = [1, 2, 3, 4, 5]\n","b = a[:]\n","b[3] = 'four'\n","print(a)"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["[1, 2, 3, 4, 5]\n"]}]},{"cell_type":"code","metadata":{"scrolled":true,"id":"iOFCswoemkF3","outputId":"8f858b2f-f732-435d-f813-5dafa013e914"},"source":["x = mystery_sequence(size=30)\n","print(x)"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["[3, 2, 3, 2, 2, 1, 0, 2, 6, 1, 3, 3, 1, 3, 0, 0, 3, 2, 1, 1, 2, 3, 2, 3, 0, 2, 1, 3, 2, 3]\n"]}]},{"cell_type":"code","metadata":{"scrolled":true,"id":"qTofXo2ImkF3","outputId":"cf68f9a8-2210-4aed-ed17-8df365405998"},"source":["for i in range(0,8):\n","    print(i, x.count(i)*'#', x.count(i))"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["0 #### 4\n","1 ###### 6\n","2 ######### 9\n","3 ########## 10\n","4  0\n","5  0\n","6 # 1\n","7  0\n"]}]},{"cell_type":"code","metadata":{"id":"TTS5R2SHmkF3","outputId":"03d1bf71-6990-40fc-f975-33662d9173b1"},"source":["x = mystery_sequence(size=50)\n","print(x)"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["[1, 1, 0, 0, 2, 4, 1, 1, 0, 0, 1, 1, 0, 1, 0, 5, 3, 2, 3, 2, 3, 3, 2, 1, 5, 3, 1, 2, 1, 2, 2, 1, 3, 2, 3, 3, 1, 2, 1, 2, 3, 2, 5, 0, 0, 2, 2, 3, 2, 0]\n"]}]},{"cell_type":"code","metadata":{"id":"vJPtHuwMmkF4","outputId":"dba6da54-8e00-4384-d290-3a9909eea5fc"},"source":["for i in range(0,8): \n","    print(i, x.count(i)*'#', x.count(i))"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["0 ######## 8\n","1 ########### 11\n","2 ############# 13\n","3 ############# 13\n","4 ## 2\n","5 # 1\n","6 # 1\n","7  0\n"]}]},{"cell_type":"code","metadata":{"id":"FN-vFcUZmkF4"},"source":["def mean(x):\n","    if x:\n","        s = 0.\n","        for i in range(0,len(x)):\n","            s = s + x[i]\n","        return s / len(x)\n","    else: \n","        return 0\n","    \n","def var(x):\n","    if len(x)>1: \n","        mu = mean(x)\n","        s = 0. \n","        for i in range(0,len(x)):\n","            s = s + (x[i]-mu)**2 \n","        return s / (len(x)-1)\n","    else:\n","        print('variance for an empty list or one number is not determined')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9-HTvfUUmkF4","outputId":"885340ca-9338-4f08-bac4-19d6c3fb59ff"},"source":["print(mean(x), sum(x)/len(x))"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["2.08 2.08\n"]}]},{"cell_type":"code","metadata":{"id":"conlhZnrmkF5","outputId":"9c03d1b4-7d51-4d67-d167-629fb5007eea"},"source":["print(var(x))"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["2.809795918367347\n"]}]},{"cell_type":"code","metadata":{"id":"cyuSziFVmkF5","outputId":"5ce07df3-707c-4fd4-fb65-e648858df246"},"source":["print(np.mean(x), np.var(x, ddof=1))"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["3.85 3.66923076923077\n"]}]},{"cell_type":"markdown","metadata":{"id":"9Rt2ABYgmkF5"},"source":["### <font color='darkblue'>Expectation value of a probability distribution:</font> \n","\n","Expectation value of some outcome $x$ which has probability distribution $p(x)$ is defined in general as \n","\n","$$E(x) = \\sum_i x_i p(x_i)$$\n","\n","where the sum is over all possible  $x$ individual outcomes. "]},{"cell_type":"markdown","metadata":{"id":"sXmTxi2kmkF5"},"source":["Expectation is the most basic and one of the most important properties of probability distribution. The name derives from the fact that this quantity gives *expected* value of $x$ if we get future outcomes of the random events distributed with $p(x)$.  "]},{"cell_type":"markdown","metadata":{"id":"wkrDVw-ymkF6"},"source":["**_For example_**, if probability to rain on a given day is $p=0.26$, and we denote outcomes $x_1=1$ if it rains and $x_2=0$ if it does not rain, the expectation value for rain  is \n","\n","$$E({\\rm rain}) = 1\\times p + 0\\times (1-p)=p,$$\n","\n","which makes sense because probability is a measure of our expectation for the plausibility of rain for that day. \n","\n","Another example: expectation value for a number one gets in a 6-sided dice roll: \n","\n","$$E({\\rm roll}) = 1\\times\\frac{1}{6} + 2\\times\\frac{1}{6}+3\\times\\frac{1}{6}+4\\times\\frac{1}{6}+5\\times\\frac{1}{6}+6\\times\\frac{1}{6} = 3.5.$$\n","\n","Of course, one does not get 3.5 in a dice roll. This number for a discrete outcome situation like this one means that expected value is either 3 or 4 with equal probability. "]},{"cell_type":"code","metadata":{"id":"qHH6Y-mjmkF6","outputId":"3825696e-c1e8-4d36-ced4-a6f63df857b4"},"source":["(1 + 2 + 3 + 4 + 5 + 6) / 6"],"execution_count":null,"outputs":[{"data":{"text/plain":["3.5"]},"execution_count":1,"metadata":{},"output_type":"execute_result"}]},{"cell_type":"markdown","metadata":{"id":"qJyTQr37mkF6"},"source":["### <font color='darkblue'>Linearity of the expectation value:</font> \n","\n","The expectation value of the sum of two random variables is equal to the sum of their individual expectation values: \n","\n","$$E(x+y) = E(x) + E(y).$$\n"]},{"cell_type":"markdown","metadata":{"id":"6n04WB4kmkF6"},"source":["What is the expectation value $E(x+y)$? Imagine we get a pair of random outcomes $x_1$ and $y_1$, then another pair $x_2$ and $y_2$, and so on until we get $n$ pairs. \n","For each pair $i$ we compute sum $x_i+y_i$ of the corresponding random outcomes. $E(x+y)$ is the expectation value of these sums. "]},{"cell_type":"markdown","metadata":{"id":"LTfbQ47vmkF6"},"source":["**Example:** throwing two dice at the same time we will get two numbers each time. The expectation value of the sum of these two values will be \n","\n","$$E({\\rm roll1}+{\\rm roll2}) = E({\\rm roll1}) + E({\\rm roll2}) = 3.5 + 3.5 = 7$$\n","\n","So expectation value for the sum of the two numbers we get on a double dice rol is 7. \n","\n","\n","We don't need to have similar processes however. For example, if I throw one dice and flip a coin (and assign heads 1 and tails 0). The expectation number for the sum of the number I get on the dice and outcome of the coin flip will be \n","\n","$$E({\\rm roll}+{\\rm flip}) = E({\\rm roll}) + E({\\rm flip}) = 3.5 + 0.5 = 4$$\n"]},{"cell_type":"markdown","metadata":{"id":"c5fNypD1mkF6"},"source":["**Example:** (from [here]()) what is the probability that someone wins a lottery, when winning means picking the right 5 numbers out of 50 numbers from 1 to 50? Assume that order of numbers does not matter and all participants in the lottery are picking number combinations randomly (that is there is no coordination). \n","\n","We know that in this case there are \n","\n","$$m=\\frac{50!}{5!(50-5)!}=2118760$$ \n","\n","combinations of 5 numbers. "]},{"cell_type":"code","metadata":{"id":"ddlnQF1ImkF7","outputId":"28b35494-8231-4cdb-f197-9f58f07810fc"},"source":["50*49*48*47*46/(5*4*3*2)"],"execution_count":null,"outputs":[{"data":{"text/plain":["2118760.0"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}]},{"cell_type":"markdown","metadata":{"id":"403ko1Z2mkF7"},"source":["If we denote $x_i=1$ the outcome that a given combination $i$ is *not* picked by *any* participant and $x_i=0$ that it is picked, the probability of picking a given combination by a single person is $1/m$, and probability of not picking it is $(1-1/m)$. The probability of combination $i$ not being picked by $n$ participants is thus $(1-1/m)^n$. So expectation value for $x_i$ is\n","\n","$$E(x_i) = 1\\times\\left(1-\\frac{1}{m}\\right)^n + 0\\times{1\\over m^n} = \\left(1-\\frac{1}{m}\\right)^n$$\n","\n","The expectation value for the number of unchosen combinations, $\\sum\\limits_{i=1}^m x_i$ (all chosen ones have $x_i=0$ and don't contribute to the sum) is \n","\n","$$E\\left[\\sum\\limits_{i=1}^m x_i\\right] = \\sum\\limits_{i=1}^m E(x_i) = m\\cdot \\left(1-{1\\over m}\\right)^n$$. \n","\n","The probability that the right combination will not be picked is thus: \n","\n","$$\\frac{m\\cdot \\left(1-{1\\over m}\\right)^n}{m} = \\left(1-{1\\over m}\\right)^n,$$\n","\n","while the complement probability that it will  be picked (that is someone wins) is given by \n","\n","$$1- \\left(1-{1\\over m}\\right)^n$$\n","\n","However, can we compute this when $m$ and $n$ are millions? "]},{"cell_type":"code","metadata":{"id":"1osqVlmamkF7","outputId":"a2a626b9-b078-47c7-e410-84e260173e57"},"source":["m = 2118760\n","n = 1000000\n","1 - (1-1/m)**n # Yes, we can..."],"execution_count":null,"outputs":[{"data":{"text/plain":["0.376230446992902"]},"execution_count":24,"metadata":{},"output_type":"execute_result"}]},{"cell_type":"markdown","metadata":{"id":"t9sB4FirmkF7"},"source":["**Formal proof.** \n","\n","If we assume that joint probability distribution of outcomes $x_i$ and $y_j$ is $p(x_i,y_j)$, then \n","\n","\\begin{eqnarray}\n","E(x+y) &=& \\sum_i \\sum_j p(x_i,y_j) (x_i +y_j) = \\sum_i \\sum_j p(x_i,y_j) x_i +\\sum_j\\sum_i p(x_i,y_j) y_j = \\sum_i x_i \\sum_j p(x_i,y_j) +\\sum_j y_j\\sum_i p(x_i,y_j) \\\\\n","&=& \\sum_i x_i p(x_i) +\\sum_j y_jp(y_j)= E(x) + E(y),\n","\\end{eqnarray}\n","\n","where in the last step $p(x_i)=\\sum_j p(x_i,y_j)$ and $p(y_j) = \\sum_i p(x_i,y_j)$. This type of summation where probabilities are summed for all possible values of one of the outcomes is called *marginalization*. "]},{"cell_type":"markdown","metadata":{"id":"XqblOOtPmkF7"},"source":["**Intuitive, but not a formal proof.** \n","As $n\\rightarrow\\infty$ the *sample mean* of these sums should converge to the expectation value (see below): \n","\n","\n","$$E(x+y)=\\frac{1}{n}\\,\\sum\\limits_{i=1}^n (x_i+y_i) = \\frac{1}{n}\\,\\sum\\limits_{i=1}^n x_i + \\frac{1}{n}\\,\\sum\\limits_{i=1}^n y_i$$\n"]},{"cell_type":"markdown","metadata":{"id":"3i9izlzvmkF7"},"source":["### <font color='darkblue'>Generalization of the expectation linearity:</font> \n","\n","In general we can show using similar reasoning that \n","\n","$$E(c_1 x_1 + c_2 x_2 +\\ldots +c_n x_n) = c_1 E(x_1) + c_2E(x_2) +\\ldots + c_n E(x_n)$$"]},{"cell_type":"markdown","metadata":{"id":"OPhAMxNimkF7"},"source":["### <font color='darkblue'>The expectation value and sample mean:</font> \n","\n","Consider the case where we have $N$ total possible outcomes $\\{x_i\\}$ where $i$ changes from 1 to $N$ and these can described by the probability distribution $p(x)$. Let's consider the expectation value of \n","\n","$$\\langle x\\rangle = \\frac{x_1+x_2+\\ldots x_N}{N}$$\n","\n","By the linearity of expected value\n","\n","$$E(\\langle x\\rangle)=\\left(\\frac{x_1+x_2+\\ldots x_N}{N}\\right) = {1\\over N}\\left[E(x_1) + E(x_2)+\\ldots E(x_N)\\right]$$ \n","\n","If all these outcomes are result of a processed described by $p(x)$ they all share the same expectation value of any outcome of this distribution $E(x_i)=E(x_j)=E(x)$, So \n","\n","$$E(\\langle x\\rangle) = {1\\over N}\\,NE(x) = E(x)$$\n","\n","So expectation value for $\\langle x\\rangle$ is equal to the expectation value of the distribution. "]},{"cell_type":"markdown","metadata":{"id":"o02fSLE8mkF7"},"source":["If we consider a subset of $0< n<N$ outcomes out of $N$ and look at the expectation value of \n","\n","$$\\bar{x} = \\frac{x_1+x_2+\\ldots +x_n}{n}$$\n","\n","We get the same expectation value for $\\bar{x}$ for a subsample of all outcomes as for all of the outcomes: \n","\n","$$E(\\bar{x})=\\left(\\frac{x_1+x_2+\\ldots x_n}{n}\\right) = {1\\over n}\\left[E(x_1) + E(x_2)+\\ldots E(x_n)\\right]={1\\over n} nE(x)=E(x)$$ \n","\n","So on average values of $\\bar{x}$ for a specific sample of outcome are expected to be close to $E(x)$. \n"]},{"cell_type":"markdown","metadata":{"id":"2AVX1sxhmkF7"},"source":["### <font color='darkblue'>The sample mean:</font> \n","\n","\n","This means that we can use $\\bar{x}$ called **_the sample mean_** given a sample of outcomes as an approximate estimate of the expectation value $E(x)$ for the probability distribution that describes these outcomes. \n","\n","$$\\bar{x} = \\frac{x_1+x_2+\\ldots +x_n}{n}\\approx E(x)$$\n","\n","\n","<p>\n","<center><img align=center width=800 src=\"https://upload.wikimedia.org/wikipedia/commons/0/0c/Determination_of_the_rute_and_the_feet_in_Frankfurt.png\"></img>\n"]},{"cell_type":"markdown","metadata":{"id":"OzzufhBOmkF7"},"source":["### <font color='darkblue'>Let's implement functions to compute sample mean and variance:</font> \n"]},{"cell_type":"code","metadata":{"id":"7Q4TaeHimkF8"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1_wJBYaRmkF8"},"source":["### <font color='darkblue'>Function to compute sample variance:</font> \n"]},{"cell_type":"code","metadata":{"id":"-cSaD7W7mkF8"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fgBuNt7XmkF8"},"source":["### <font color='darkblue'>Expectation value of the product of *independent* outcomes:</font> \n","\n","$$E(x\\cdot y)=E(x) \\cdot E(y)$$"]},{"cell_type":"markdown","metadata":{"id":"YslBSt3HmkF8"},"source":["**Proof.** Probabilities of independent random outcomes are multiplied to get the probability of both outcomes occuring. The product of a pair of random outcomes $x_i$ and $y_i$ is itself a random outcome $x_i y_i$. \n","\n","So by definition of expectation\n","\n","$$E(x\\cdot y) = \\sum_i \\sum_j p(x_i,y_j) x_i y_j =  \\sum_i \\sum_j p(x_i) p(y_j) x_i y_j= \\sum_i p(x_i)x_i\\, \\sum_j p(y_j) y_j = E(x)\\cdot E(y),$$\n","\n","where the fact that joint probability of independent events is equal to the product of individual probabilities: $p(x_i,y_j)=p(x_i)p(y_j)$. "]},{"cell_type":"markdown","metadata":{"id":"26oSCMc7mkF8"},"source":["### <font color='darkblue'>Expectation value of the binomial distribution:</font> \n","\n","I we consider expectation of a single outcome in a given trial/experiment, where outcome we are evaluating probability for corresponds to $x_t=1$ and its complement (the outcome not occuring) is $x_t=0$, it is given simply by: \n","\n","$$E(x_t) = 1\\times p + 0\\times (1-p) = p. $$\n","\n","Let $x$ denote the total of $N$ outcomes: $x = x_{t1} + x_{t2} +\\ldots + x_{tN}$. \n","\n","\n","By the linearity property of the expectation: \n","\n","$$E(x) = E(x_{t1} + x_{t2} +\\ldots + x_{tN}) = E(x_{t1}) + E(x_{t2}) +\\ldots + E(x_{tN})=Np.$$\n","\n","Thus, the value of $x$ we expect *on average* when we repeat such $N$ trials is $Np$. "]},{"cell_type":"markdown","metadata":{"id":"Gq3fNJAYmkF8"},"source":["**_Note_**: $x_t$ introduced above is an example of what's called a *random* variable. "]},{"cell_type":"markdown","metadata":{"id":"XeVV-mswmkF8"},"source":["**_Note_**: We can see here the basis for estimating probability via frequency. If we measure average number of  outcomes in repeated experiments to be $\\bar{x}$ and assume that it approximates the true expectation, $\\bar{x}\\approx E(x)$, we have \n","\n","$$p \\approx \\frac{\\bar{x}}{N}.$$"]},{"cell_type":"markdown","metadata":{"id":"ZI7faiNdmkF8"},"source":["### <font color='darkblue'>A note on notation:</font> \n","\n","Expectation of random outcome $x$ is usually denoted as $E(x)$ in statistics textbooks, but in practical statistical application it is often denoted as $\\mu_x$ or just $\\mu$ if there is no confusion with other variables. \n","\n","The two notation can be used together, as notation like $E(E(x) +E(y))$ often becomes difficult to read.  \n","\n","\n","So keep in mind when you read further that \n","\n","$$E(x) = \\mu_x = \\mu$$"]},{"cell_type":"markdown","metadata":{"id":"KVp82ETtmkF8"},"source":["### <font color='darkblue'>Variance of a probability distribution:</font> \n","\n","is defined as expectation of $[x-E(x)]^2$:\n","\n","$${\\rm Var}(x) = E([x-E(x)]^2) = E(x^2 - 2xE(x) + E(x)^2) = E(x^2) -2E(x)^2 + E(x)^2 = E(x^2) - E(x)^2$$\n","\n","is another important property of a probability distribution. "]},{"cell_type":"markdown","metadata":{"id":"3SNuqaUMmkF8"},"source":["Note that $E(x)$ is just a constant (a number), so $E(E(x))=E(x)$, which was used above. "]},{"cell_type":"markdown","metadata":{"id":"jeSFMwjpmkF8"},"source":["### <font color='darkblue'>Linearity of the variance:</font> \n","\n","If $x$ and $y$ are *independent* random outcomes \n","\n","$${\\rm Var}(x+y) = {\\rm Var}(x) + {\\rm Var}(y).$$"]},{"cell_type":"markdown","metadata":{"id":"FmmB6FMtmkF8"},"source":["${\\rm Var}(x+y)$ is the variance of the sums of two outcomes of two different and independent random processes. "]},{"cell_type":"markdown","metadata":{"id":"288cGr7LmkF8"},"source":["**Proof.** Recall that $E(x+y) = E(x) +E(y)=\\mu_x + \\mu_y.$\n","\n","\\begin{eqnarray}\n","{\\rm Var}(x+y)& =& E\\left[\\left((x+y) - (\\mu_x+\\mu_y)\\right)^2\\right] = E\\left[\\left((x-\\mu_x)+(y-\\mu_y)\\right)^2  \\right]\\\\\n","&=& E\\left[\\left(x-\\mu_x\\right)^2\\right] + 2E\\left[(x-\\mu_x)(y-\\mu_y)\\right] + E\\left[\\left(y-\\mu_y\\right)^2\\right]\\\\\n","\\end{eqnarray}\n","\n","The first and third terms are ${\\rm Var}(x)$ and ${\\rm Var}(y)$, while the middle for independent random outcomes is equal to the product: \n","\n","$$2E\\left[(x-\\mu_x)(y-\\mu_y)\\right]=2E(x-\\mu_x) E(y-\\mu_y).$$\n","\n","Applying linearity again $E(x-\\mu_x) = E(x) - E(\\mu_x)$, where $E(x)=\\mu_x$ by definition and $E(\\mu_x)=\\mu_x$ because $\\mu_x$ is just a constant, so $E(x-\\mu_x)$ and $E(y-\\mu_y)$ (for similar reasons) are both zeros. The middle term is thus zero and we proved linearity of the variance.  "]},{"cell_type":"markdown","metadata":{"id":"9S848EiqmkF9"},"source":["### <font color='darkblue'>Linearity of the variance: another manifestation</font> \n","\n","$${\\rm Var}(x-y) = {\\rm Var}(x) + {\\rm Var}(y).$$"]},{"cell_type":"markdown","metadata":{"id":"52wOmgdCmkF9"},"source":["The proof of this is similar to the proof above. The result does not depend on the sign of $x$ or $y$ because they are squared in the calculation of variance. "]},{"cell_type":"markdown","metadata":{"id":"uKLHXzYEmkF9"},"source":["**_Important!_** The linearity is only true for **_independent_** random variables that do not fluctuate in a correlated manner. Thus, for example ${\\rm Var}(x+x)$ is not equal to ${\\rm Var}(x) + {\\rm Var}(x)=2{\\rm Var}(x)$ because $x$ is perfectly dependent on itself. In the case of dependent (*correlated*) random variables a different expression, taking into account correlation should be used. We will come back to this later in the course. "]},{"cell_type":"markdown","metadata":{"id":"TJXc4tY3mkF9"},"source":["**_Note:_** The linearity of variance of independent random numbers - or that when we are dealing with a sum of outcomes of two random processes the variance of the sum is the sum of variances - is a *very* important property to remember. This property is used often in statistics. "]},{"cell_type":"markdown","metadata":{"id":"y5RI2dAdmkF9"},"source":["**Example:** when we are collecting photons with a [CCD camera](http://spiff.rit.edu/classes/phys445/lectures/ccd1/ccd1.html) from some part of the sky, photons generally come from the background light (scattered light in the Earth atmosphere and unresolved light of distant objects) that is approximately the same in different regions of the sky and photons from astronomical sources. We are usually interested in the latter, but in the pixels corresponding to the source photons from both background light and the source will be received. \n","\n","The variance of photon counts in these pixels will thus be given by the sum of variances of background light and the source: ${\\rm Var}(n_b + n_s) = {\\rm Var}(n_b) + {\\rm Var}(n_s)$. Knowledge of this is very handy in the estimates of *signal-to-noise ratio* of the source detections. More on this later. "]},{"cell_type":"markdown","metadata":{"id":"NORf8w-zmkF9"},"source":["**_Note:_** it is very important to remember that linearity property was derived for *independent* random outcomes. For dependent (or *correlated*) random outcomes linearity does not apply and different expression for ${\\rm Var}(x+y)$ applies. We will discuss it when we will talk about correlations. "]},{"cell_type":"markdown","metadata":{"id":"wztRuHSDmkF9"},"source":["### <font color='darkblue'>Variance of binomial probability distribution:</font> \n","\n","\n","Variance of a single outcome in a given trial/experiment, where outcome we are evaluating probability for corresponds to $x_t=1$ and its complement (the outcome not occuring) is $x_t=0$, is then \n","\n","$${\\rm Var}(x_t) = E(x^2_t) - E(x_t)^2 = 1^2p + 0^2 (1-p) - p^2 = p(1-p).$$\n","\n","Like expectations, variance also has linearity property, so variance of the total outcome of $N$ trials is \n","\n","$${\\rm Var}(x) = {\\rm Var}(x_{t1} + x_{t2} +\\ldots + x_{tN}) = {\\rm Var}(x_{t1}) + {\\rm Var}(x_{t2}) +\\ldots + {\\rm Var}(x_{tN})=N\\,{\\rm Var}(x_t) = Np(1-p).$$\n"]},{"cell_type":"markdown","metadata":{"id":"E6Z1mR8CmkF9"},"source":["### <font color='darkblue'>*Standard deviation* of a distribution:</font> \n","\n","The *root mean square* (*rms*), also called *standard deviation* of a distribution is \n","\n","$$\\sigma(x) = \\sqrt{{\\rm Var}(x)}$$\n","\n","and describes the characteristic *width* of a distribution about its mean value. "]},{"cell_type":"markdown","metadata":{"id":"Dft9dGC_mkF9"},"source":["### <font color='darkblue'>Standard deviation of a binomial distribution:</font> \n","\n","For $N$ trials when considering outcomes occuring with probability $p$: \n","\n","$$\\sigma(x) = \\sqrt{N p(1-p)}$$"]},{"cell_type":"markdown","metadata":{"id":"fh-cA01JmkF9"},"source":["**_Note_**: that $\\sigma$ here is proportional to $\\sqrt{N}$. \n","\n","If we consider the ratio of the expectation value to $\\sigma$ we get: \n","\n","$$\\frac{E(x)}{\\sigma(x)} = \\frac{Np}{\\sqrt{N p(1-p)}} = \\sqrt{\\frac{Np}{1-p}}.$$\n","\n","\n","This is also characteristic of another commonly encountered *Poisson probability distribution*, which is binomial distribution in the limit of large $N$ and small $p$.  "]},{"cell_type":"markdown","metadata":{"id":"7-AN6T5GmkF9"},"source":["### <font color='darkblue'>A note on notation:</font> \n","\n","Variance of random outcome $x$ is often denoted as ${\\rm Var}(x)$ in statistics textbooks, but in practical statistical application it is often denoted as $\\sigma^2_x$ or just $\\sigma^2$ if there is no confusion with other variables because it is square of the standard deviation, which is most often denoted as $\\sigma_x$ or just $\\sigma$. "]},{"cell_type":"markdown","metadata":{"id":"t14BqrfDmkF9"},"source":["### <font color='darkblue'>Expectation value and variance are equal for the Poisson distribution</font>\n","\n","Expectation value is $\\lambda$ by definition because we used it to denote expectation of the binomial distribution and Poisson distribution is but a limiting case of the same distribution. \n","\n","The same is true for the variance, ${\\rm Var}(n) = Np(1-p)$, which in this limit $p=\\lambda/N\\rightarrow 0$ is: \n","\n","$${\\rm Var}(n) = Np=\\lambda.$$\n","\n","So expectation value and variance of the Poisson distribution are both $\\lambda$. "]},{"cell_type":"markdown","metadata":{"id":"ZLTq4f7hmkF9"},"source":["**_Note_**: this is great we have only one number to remember for the expectation and variance, and one less thing to worry about! "]},{"cell_type":"markdown","metadata":{"id":"NVp69GaPmkF9"},"source":["### <font color='darkblue'>Example: signal-to-noise ratio (SNR) of a source detection</font>\n","\n","Astronomical images recorded by a CCD camera usually consist of source pixels (pixels in which light from astronomical objects is recorded) and \"background pixels\" that lie between sources. Both source and background pixels receive photons of light at the same rate due to approximately uniform \"sky background light\". This background light arises from scattering of light by the atmosphere and unresolved light from distant faint sources fills the sky with an approximately uniform \"background\" (see an example image above). The latter creates sky background light even when observations are made from space outside of the Earth atmosphere. \n"]},{"cell_type":"markdown","metadata":{"id":"gXM6aQzpmkF9"},"source":["A part of an [image](https://esahubble.org/images/heic0714f/) from the [Hubble Space Telescope](https://hubblesite.org/), which shows a very distant faint galaxy at the center. The reddish pixels of that galaxy (the source of these photons) are surrounded by background pixels that contains sky background photons. If we want to measure brightness of the source by summing its photons, the uncertainty for this estimate are the photons of the sky background that hit the source pixels and contribute to the photon count in these pixels. \n","\n","<p>\n","<center>\n","<img width=400 src=\"https://cdn.spacetelescope.org/archives/images/large/heic0714f.jpg\"></img></center>"]},{"cell_type":"markdown","metadata":{"id":"jPTE7zjjmkF9"},"source":["So source pixels receive photons from both the source and this \"sky background\", while background pixels receive only the sky background photons. \n","\n","If in a given exposure time $\\tau$ pixels associated with a source received $n_{\\rm tot}$ photons, some of these came from the source and some from the sky background. \n","\n","If we isolate background pixels in an image we can measure the average number of photons per pixel that come from the sky background by averaging photon counts in them. We can use this average to estimate the number of background sky photons in the pixels associated with the source we are interested in, $n_b$. \n","\n","Our best estimate for the number of photons that actually come from the source is \n","\n","$$\\hat{n}_s = n_{\\rm tot} - n_b.$$\n","\n","The symbol $\\hat{}$ in statistics usually means \"the estimate of\" (as opposed to the true value of what we are estimating). \n","\n","According to the linearity of variance \n","\n","$$\\sigma^2(\\hat{n}_s) = \\sigma^2(n_{\\rm tot}) + \\sigma^2(n_b)$$ \n","\n","and the standard deviation, which is usually used as a measure of uncertainty of the estimate, us $\\sigma(\\hat{n}_s)=\\sqrt{\\sigma^2(\\hat{n}_s)}$. \n","\n","If the photons from the source and sky background arrive at discrete times with constant rates, the distribution of photon counts in a pixel or in a collection of pixels should be described by the Poisson distribution with the expectation value roughly $n_{\\rm tot}$ (source pixels) or $n_b$ (background pixels). \n","\n","Given that for the Poisson distribution expectation value is equal to the variance, we can estimate variances just from the counts: \n","\n","$$ \\sigma^2(n_{\\rm tot})\\approx n_{\\rm tot};\\ \\ \\ \\ \\ \\sigma^2(n_b)\\approx n_b.$$\n"]},{"cell_type":"markdown","metadata":{"id":"cZvMGkTemkF-"},"source":["**_The Signal-to-Noise Ratio (SNR)_** of a source is defined as\n","\n","$${\\rm SNR} = \\frac{\\hat{n}_s}{\\sigma(\\hat{n}_s)} = \\frac{\\hat{n}_s}{\\sqrt{\\sigma^2(\\hat{n}_{\\rm tot}) + \\sigma^2(\\hat{n}_b)}} \\approx \\frac{n_{\\rm tot}-n_b}{\\sqrt{n_{\\rm tot} + n_b}}.$$\n","\n","\n","SNR allows us to estimate how much the source \"signal\" stands out relative to the noise due to the sky background light. "]},{"cell_type":"markdown","metadata":{"id":"RBVwfwqxmkF-"},"source":["In practice, SNR is used to determine the plausibility that a given candidate source is real. You would need to know SNR for any source that you claim to have discovered to convince your colleagues. \n","\n","Usually, sources with ${\\rm SNR}\\geq 5$ are considered real, although sometimes sources with ${\\rm SNR}\\sim 3-5$ are also analyzed or followed up observationally. "]},{"cell_type":"code","metadata":{"id":"sW-JdxW2mkF-"},"source":[""],"execution_count":null,"outputs":[]}]}